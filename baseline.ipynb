{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Z6l4B_ey7cyx",
      "metadata": {
        "id": "Z6l4B_ey7cyx"
      },
      "source": [
        "# НТО ИИ 2025-2026: Индивидуальный этап (Baseline)\n",
        "\n",
        "## Описание проекта\n",
        "Данный ноутбук представляет собой baseline-решение задачи предсказания оценок книг. Мы решаем задачу регрессии, используя алгоритм LightGBM и корректную временную валидацию.\n",
        "\n",
        "### Структура решения:\n",
        "1. **Подготовка данных**: загрузка, очистка, генерация признаков (текстовые эмбеддинги BERT, TF-IDF, метаданные).\n",
        "2. **Обучение**: тренировка модели на исторических данных с валидацией на \"будущих\" данных.\n",
        "3. **Предсказание**: генерация прогноза для тестовой выборки.\n",
        "\n",
        "> **Примечание:** Все файлы проекта объединены в этом ноутбуке для удобства использования в Google Colab / Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "kYHA-BKz7cyy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYHA-BKz7cyy",
        "outputId": "7c86520c-6604-4880-efee-425e7bb4ef00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# === УСТАНОВКА ЗАВИСИМОСТЕЙ ===\n",
        "!pip install lightgbm transformers sentencepiece pandas scikit-learn torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "awWDtEWclfWg",
      "metadata": {
        "id": "awWDtEWclfWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c18ee2-f47d-40cc-e70f-7abc23e2b435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac06c74f",
      "metadata": {
        "id": "ac06c74f"
      },
      "outputs": [],
      "source": [
        "# === ИМПОРТ БИБЛИОТЕК ===\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import joblib\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import lightgbm as lgb\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Фиксация случайности для воспроизводимости\n",
        "def seed_everything(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a9bb4a",
      "metadata": {
        "id": "93a9bb4a"
      },
      "source": [
        "## 1. Конфигурация и Константы\n",
        "Здесь определены пути к файлам, названия колонок и гиперпараметры модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "23ab7315",
      "metadata": {
        "id": "23ab7315"
      },
      "outputs": [],
      "source": [
        "class constants:\n",
        "    # --- FILENAMES ---\n",
        "    TRAIN_FILENAME = \"train.csv\"\n",
        "    TEST_FILENAME = \"test.csv\"\n",
        "    USER_DATA_FILENAME = \"users.csv\"\n",
        "    BOOK_DATA_FILENAME = \"books.csv\"\n",
        "    BOOK_GENRES_FILENAME = \"book_genres.csv\"\n",
        "    GENRES_FILENAME = \"genres.csv\"\n",
        "    BOOK_DESCRIPTIONS_FILENAME = \"book_descriptions.csv\"\n",
        "    SUBMISSION_FILENAME = \"submission.csv\"\n",
        "    TFIDF_VECTORIZER_FILENAME = \"tfidf_vectorizer.pkl\"\n",
        "    BERT_EMBEDDINGS_FILENAME = \"bert_embeddings.pkl\"\n",
        "    BERT_MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
        "    PROCESSED_DATA_FILENAME = \"processed_features.parquet\"\n",
        "\n",
        "    # --- COLUMN NAMES ---\n",
        "    # Main columns\n",
        "    COL_USER_ID = \"user_id\"\n",
        "    COL_BOOK_ID = \"book_id\"\n",
        "    COL_TARGET = \"rating\"\n",
        "    COL_SOURCE = \"source\"\n",
        "    COL_PREDICTION = \"rating_predict\"\n",
        "    COL_HAS_READ = \"has_read\"\n",
        "    COL_TIMESTAMP = \"timestamp\"\n",
        "\n",
        "    # Feature columns (newly created)\n",
        "    F_USER_MEAN_RATING = \"user_mean_rating\"\n",
        "    F_USER_RATINGS_COUNT = \"user_ratings_count\"\n",
        "    F_BOOK_MEAN_RATING = \"book_mean_rating\"\n",
        "    F_BOOK_RATINGS_COUNT = \"book_ratings_count\"\n",
        "    F_AUTHOR_MEAN_RATING = \"author_mean_rating\"\n",
        "    F_BOOK_GENRES_COUNT = \"book_genres_count\"\n",
        "\n",
        "    # Metadata columns from raw data\n",
        "    COL_GENDER = \"gender\"\n",
        "    COL_AGE = \"age\"\n",
        "    COL_AUTHOR_ID = \"author_id\"\n",
        "    COL_PUBLICATION_YEAR = \"publication_year\"\n",
        "    COL_LANGUAGE = \"language\"\n",
        "    COL_PUBLISHER = \"publisher\"\n",
        "    COL_AVG_RATING = \"avg_rating\"\n",
        "    COL_GENRE_ID = \"genre_id\"\n",
        "    COL_DESCRIPTION = \"description\"\n",
        "\n",
        "    # --- VALUES ---\n",
        "    VAL_SOURCE_TRAIN = \"train\"\n",
        "    VAL_SOURCE_TEST = \"test\"\n",
        "\n",
        "    # --- MAGIC NUMBERS ---\n",
        "    MISSING_CAT_VALUE = \"-1\"\n",
        "    MISSING_NUM_VALUE = -1\n",
        "    PREDICTION_MIN_VALUE = 0\n",
        "    PREDICTION_MAX_VALUE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a0ae9f57",
      "metadata": {
        "id": "a0ae9f57"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    # --- DIRECTORIES ---\n",
        "    # Настраиваем пути для работы в текущей директории (Colab/Kaggle)\n",
        "    ROOT_DIR = Path(\".\")\n",
        "    DATA_DIR = ROOT_DIR / \"/content/drive/MyDrive/data\"\n",
        "    RAW_DATA_DIR = DATA_DIR / \"/content/drive/MyDrive/data/raw\"\n",
        "    INTERIM_DATA_DIR = DATA_DIR / \"interim\"\n",
        "    PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
        "    OUTPUT_DIR = ROOT_DIR / \"output\"\n",
        "    MODEL_DIR = OUTPUT_DIR / \"models\"\n",
        "    SUBMISSION_DIR = OUTPUT_DIR / \"submissions\"\n",
        "\n",
        "    # --- PARAMETERS ---\n",
        "    RANDOM_STATE = 42\n",
        "    TARGET = constants.COL_TARGET\n",
        "\n",
        "    # --- TEMPORAL SPLIT CONFIG ---\n",
        "    # 0.8 means 80% of data points (by timestamp) go to train, 20% to validation\n",
        "    TEMPORAL_SPLIT_RATIO = 0.8\n",
        "\n",
        "    # --- TRAINING CONFIG ---\n",
        "    EARLY_STOPPING_ROUNDS = 50\n",
        "    MODEL_FILENAME = \"lgb_model.txt\"\n",
        "\n",
        "    # --- TF-IDF PARAMETERS ---\n",
        "    TFIDF_MAX_FEATURES = 500\n",
        "    TFIDF_MIN_DF = 2\n",
        "    TFIDF_MAX_DF = 0.95\n",
        "    TFIDF_NGRAM_RANGE = (1, 2)\n",
        "\n",
        "    # --- BERT PARAMETERS ---\n",
        "    BERT_MODEL_NAME = constants.BERT_MODEL_NAME\n",
        "    BERT_BATCH_SIZE = 8\n",
        "    BERT_MAX_LENGTH = 512\n",
        "    BERT_EMBEDDING_DIM = 768\n",
        "    BERT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    BERT_GPU_MEMORY_FRACTION = 0.75\n",
        "\n",
        "    # --- FEATURES ---\n",
        "    CAT_FEATURES = [\n",
        "        constants.COL_USER_ID,\n",
        "        constants.COL_BOOK_ID,\n",
        "        constants.COL_GENDER,\n",
        "        constants.COL_AGE,\n",
        "        constants.COL_AUTHOR_ID,\n",
        "        constants.COL_PUBLICATION_YEAR,\n",
        "        constants.COL_LANGUAGE,\n",
        "        constants.COL_PUBLISHER,\n",
        "    ]\n",
        "\n",
        "    # --- MODEL PARAMETERS ---\n",
        "    LGB_PARAMS = {\n",
        "        \"objective\": \"rmse\",\n",
        "        \"metric\": \"rmse\",\n",
        "        \"verbose\": -1,\n",
        "        \"n_jobs\": -1,\n",
        "        \"seed\": RANDOM_STATE,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"learning_rate\": 0.015,\n",
        "        \"n_estimators\": 8000,\n",
        "        \"num_leaves\": 511,\n",
        "        \"max_depth\": -1,\n",
        "        \"min_data_in_leaf\": 20,\n",
        "        \"min_child_weight\": 0.001,\n",
        "        \"feature_fraction\": 0.65,\n",
        "        \"bagging_fraction\": 0.65,\n",
        "        \"bagging_freq\": 7,\n",
        "        \"lambda_l1\": 0.5,\n",
        "        \"lambda_l2\": 1.0,\n",
        "        \"min_split_gain\": 0.01,\n",
        "        \"max_bin\": 511,\n",
        "        \"extra_trees\": True,\n",
        "        \"max_delta_step\": 0.5,\n",
        "        \"min_data_per_group\": 50,\n",
        "        \"cat_smooth\": 10,\n",
        "        \"top_rate\": 0.2,\n",
        "        \"other_rate\": 0.1,\n",
        "        \"boosting\": \"rf\",\n",
        "        \"bagging_freq\": 1,\n",
        "        }\n",
        "\n",
        "    LGB_FIT_PARAMS = {\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"callbacks\": [],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9395b9b4",
      "metadata": {
        "id": "9395b9b4"
      },
      "source": [
        "## 2. Вспомогательные утилиты (Utils)\n",
        "Функции для временного разделения данных и загрузки датасетов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7dde2e86",
      "metadata": {
        "id": "7dde2e86"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Iterate through all the columns of a dataframe and modify the data type\n",
        "    to reduce memory usage.\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object and col_type.name != \"category\" and \"datetime\" not in col_type.name:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
        "    print(f\"Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Iterate through all the columns of a dataframe and modify the data type\n",
        "    to reduce memory usage.\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object and col_type.name != \"category\" and \"datetime\" not in col_type.name:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
        "    print(f\"Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- TEMPORAL SPLIT UTILS ---\n",
        "\n",
        "def temporal_split_by_date(\n",
        "    df: pd.DataFrame, split_date: pd.Timestamp, timestamp_col: str = constants.COL_TIMESTAMP\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"Splits DataFrame into train and validation sets based on absolute date threshold.\"\"\"\n",
        "    if timestamp_col not in df.columns:\n",
        "        raise ValueError(f\"Timestamp column '{timestamp_col}' not found in DataFrame.\")\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
        "        df = df.copy()\n",
        "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "\n",
        "    train_mask = df[timestamp_col] <= split_date\n",
        "    val_mask = df[timestamp_col] > split_date\n",
        "\n",
        "    if train_mask.sum() == 0:\n",
        "        raise ValueError(f\"No records found with timestamp <= {split_date}.\")\n",
        "    if val_mask.sum() == 0:\n",
        "        raise ValueError(f\"No records found with timestamp > {split_date}.\")\n",
        "\n",
        "    if train_mask.sum() > 0 and val_mask.sum() > 0:\n",
        "        max_train_timestamp = df.loc[train_mask, timestamp_col].max()\n",
        "        min_val_timestamp = df.loc[val_mask, timestamp_col].min()\n",
        "\n",
        "        if min_val_timestamp <= max_train_timestamp:\n",
        "            raise ValueError(\n",
        "                f\"Temporal split validation failed: min validation timestamp ({min_val_timestamp}) \"\n",
        "                f\"is not greater than max train timestamp ({max_train_timestamp}).\"\n",
        "            )\n",
        "\n",
        "    return train_mask, val_mask\n",
        "\n",
        "\n",
        "def get_split_date_from_ratio(\n",
        "    df: pd.DataFrame, ratio: float, timestamp_col: str = constants.COL_TIMESTAMP\n",
        ") -> pd.Timestamp:\n",
        "    \"\"\"Calculates split date based on ratio of data points.\"\"\"\n",
        "    if not 0 < ratio < 1:\n",
        "        raise ValueError(f\"Ratio must be between 0 and 1, got {ratio}\")\n",
        "\n",
        "    if timestamp_col not in df.columns:\n",
        "        raise ValueError(f\"Timestamp column '{timestamp_col}' not found in DataFrame.\")\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
        "        df = df.copy()\n",
        "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "\n",
        "    sorted_timestamps = df[timestamp_col].sort_values()\n",
        "    threshold_index = int(len(sorted_timestamps) * ratio)\n",
        "\n",
        "    return sorted_timestamps.iloc[threshold_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "07291395",
      "metadata": {
        "id": "07291395"
      },
      "outputs": [],
      "source": [
        "# --- DATA LOADING UTILS ---\n",
        "\n",
        "def load_and_merge_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Loads raw data files and merges them into a single DataFrame.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    dtype_spec: Dict[str, Any] = {\n",
        "        constants.COL_USER_ID: \"int32\",\n",
        "        constants.COL_BOOK_ID: \"int32\",\n",
        "        constants.COL_TARGET: \"float32\",\n",
        "        constants.COL_GENDER: \"category\",\n",
        "        constants.COL_AGE: \"float32\",\n",
        "        constants.COL_AUTHOR_ID: \"int32\",\n",
        "        constants.COL_PUBLICATION_YEAR: \"float32\",\n",
        "        constants.COL_LANGUAGE: \"category\",\n",
        "        constants.COL_PUBLISHER: \"category\",\n",
        "        constants.COL_AVG_RATING: \"float32\",\n",
        "        constants.COL_GENRE_ID: \"int16\",\n",
        "    }\n",
        "\n",
        "    train_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.TRAIN_FILENAME,\n",
        "        dtype={\n",
        "            k: v\n",
        "            for k, v in dtype_spec.items()\n",
        "            if k in [constants.COL_USER_ID, constants.COL_BOOK_ID, constants.COL_TARGET]\n",
        "        },\n",
        "        parse_dates=[constants.COL_TIMESTAMP],\n",
        "    )\n",
        "\n",
        "    initial_count = len(train_df)\n",
        "    train_df = train_df[train_df[constants.COL_HAS_READ] == 1].copy()\n",
        "    print(f\"Filtered training data: {initial_count} -> {len(train_df)} rows (only has_read=1)\")\n",
        "\n",
        "    test_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.TEST_FILENAME,\n",
        "        dtype={k: v for k, v in dtype_spec.items() if k in [constants.COL_USER_ID, constants.COL_BOOK_ID]},\n",
        "    )\n",
        "    user_data_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.USER_DATA_FILENAME,\n",
        "        dtype={\n",
        "            k: v for k, v in dtype_spec.items() if k in [constants.COL_USER_ID, constants.COL_GENDER, constants.COL_AGE]\n",
        "        },\n",
        "    )\n",
        "    book_data_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.BOOK_DATA_FILENAME,\n",
        "        dtype={\n",
        "            k: v\n",
        "            for k, v in dtype_spec.items()\n",
        "            if k in [\n",
        "                constants.COL_BOOK_ID, constants.COL_AUTHOR_ID, constants.COL_PUBLICATION_YEAR,\n",
        "                constants.COL_LANGUAGE, constants.COL_AVG_RATING, constants.COL_PUBLISHER,\n",
        "            ]\n",
        "        },\n",
        "    )\n",
        "    book_genres_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.BOOK_GENRES_FILENAME,\n",
        "        dtype={k: v for k, v in dtype_spec.items() if k in [constants.COL_BOOK_ID, constants.COL_GENRE_ID]},\n",
        "    )\n",
        "    genres_df = pd.read_csv(config.RAW_DATA_DIR / constants.GENRES_FILENAME)\n",
        "    book_descriptions_df = pd.read_csv(\n",
        "        config.RAW_DATA_DIR / constants.BOOK_DESCRIPTIONS_FILENAME,\n",
        "        dtype={constants.COL_BOOK_ID: \"int32\"},\n",
        "    )\n",
        "\n",
        "    print(\"Data loaded. Merging datasets...\")\n",
        "\n",
        "    train_df[constants.COL_SOURCE] = constants.VAL_SOURCE_TRAIN\n",
        "    test_df[constants.COL_SOURCE] = constants.VAL_SOURCE_TEST\n",
        "    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
        "\n",
        "    combined_df = combined_df.merge(user_data_df, on=constants.COL_USER_ID, how=\"left\")\n",
        "    book_data_df = book_data_df.drop_duplicates(subset=[constants.COL_BOOK_ID])\n",
        "    combined_df = combined_df.merge(book_data_df, on=constants.COL_BOOK_ID, how=\"left\")\n",
        "\n",
        "    print(f\"Merged data shape: {combined_df.shape}\")\n",
        "    return combined_df, book_genres_df, genres_df, book_descriptions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f214f9",
      "metadata": {
        "id": "65f214f9"
      },
      "source": [
        "## 3. Генерация признаков (Feature Engineering)\n",
        "Код разделен на логические блоки: агрегации, жанры, TF-IDF, BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "990fd000",
      "metadata": {
        "id": "990fd000"
      },
      "outputs": [],
      "source": [
        "def add_aggregate_features(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculates and adds user, book, and author aggregate features.\"\"\"\n",
        "    print(\"Adding aggregate features...\")\n",
        "\n",
        "    # User-based aggregates\n",
        "    user_agg = train_df.groupby(constants.COL_USER_ID)[config.TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
        "    user_agg.columns = [\n",
        "        constants.COL_USER_ID,\n",
        "        constants.F_USER_MEAN_RATING,\n",
        "        constants.F_USER_RATINGS_COUNT,\n",
        "    ]\n",
        "\n",
        "    # Book-based aggregates\n",
        "    book_agg = train_df.groupby(constants.COL_BOOK_ID)[config.TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
        "    book_agg.columns = [\n",
        "        constants.COL_BOOK_ID,\n",
        "        constants.F_BOOK_MEAN_RATING,\n",
        "        constants.F_BOOK_RATINGS_COUNT,\n",
        "    ]\n",
        "\n",
        "    # Author-based aggregates\n",
        "    author_agg = train_df.groupby(constants.COL_AUTHOR_ID)[config.TARGET].agg([\"mean\"]).reset_index()\n",
        "    author_agg.columns = [constants.COL_AUTHOR_ID, constants.F_AUTHOR_MEAN_RATING]\n",
        "\n",
        "    df = df.merge(user_agg, on=constants.COL_USER_ID, how=\"left\")\n",
        "    df = df.merge(book_agg, on=constants.COL_BOOK_ID, how=\"left\")\n",
        "    return df.merge(author_agg, on=constants.COL_AUTHOR_ID, how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "79f89659",
      "metadata": {
        "id": "79f89659"
      },
      "outputs": [],
      "source": [
        "def add_genre_features(df: pd.DataFrame, book_genres_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculates and adds the count of genres for each book.\"\"\"\n",
        "    print(\"Adding genre features...\")\n",
        "    genre_counts = book_genres_df.groupby(constants.COL_BOOK_ID)[constants.COL_GENRE_ID].count().reset_index()\n",
        "    genre_counts.columns = [\n",
        "        constants.COL_BOOK_ID,\n",
        "        constants.F_BOOK_GENRES_COUNT,\n",
        "    ]\n",
        "    return df.merge(genre_counts, on=constants.COL_BOOK_ID, how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d12fb535",
      "metadata": {
        "id": "d12fb535"
      },
      "outputs": [],
      "source": [
        "def add_text_features(df: pd.DataFrame, train_df: pd.DataFrame, descriptions_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Adds TF-IDF features from book descriptions.\"\"\"\n",
        "    print(\"Adding text features (TF-IDF)...\")\n",
        "\n",
        "    config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    vectorizer_path = config.MODEL_DIR / constants.TFIDF_VECTORIZER_FILENAME\n",
        "\n",
        "    train_books = train_df[constants.COL_BOOK_ID].unique()\n",
        "    train_descriptions = descriptions_df[descriptions_df[constants.COL_BOOK_ID].isin(train_books)].copy()\n",
        "    train_descriptions[constants.COL_DESCRIPTION] = train_descriptions[constants.COL_DESCRIPTION].fillna(\"\")\n",
        "\n",
        "    if vectorizer_path.exists():\n",
        "        print(f\"Loading existing vectorizer from {vectorizer_path}\")\n",
        "        vectorizer = joblib.load(vectorizer_path)\n",
        "    else:\n",
        "        print(\"Fitting TF-IDF vectorizer on training descriptions...\")\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=config.TFIDF_MAX_FEATURES,\n",
        "            min_df=config.TFIDF_MIN_DF,\n",
        "            max_df=config.TFIDF_MAX_DF,\n",
        "            ngram_range=config.TFIDF_NGRAM_RANGE,\n",
        "        )\n",
        "        vectorizer.fit(train_descriptions[constants.COL_DESCRIPTION])\n",
        "        joblib.dump(vectorizer, vectorizer_path)\n",
        "        print(f\"Vectorizer saved to {vectorizer_path}\")\n",
        "\n",
        "    all_descriptions = descriptions_df[[constants.COL_BOOK_ID, constants.COL_DESCRIPTION]].copy()\n",
        "    all_descriptions[constants.COL_DESCRIPTION] = all_descriptions[constants.COL_DESCRIPTION].fillna(\"\")\n",
        "\n",
        "    description_map = dict(\n",
        "        zip(all_descriptions[constants.COL_BOOK_ID], all_descriptions[constants.COL_DESCRIPTION], strict=False)\n",
        "    )\n",
        "\n",
        "    df_descriptions = df[constants.COL_BOOK_ID].map(description_map).fillna(\"\")\n",
        "    tfidf_matrix = vectorizer.transform(df_descriptions)\n",
        "\n",
        "    tfidf_feature_names = [f\"tfidf_{i}\" for i in range(tfidf_matrix.shape[1])]\n",
        "    tfidf_df = pd.DataFrame(\n",
        "        tfidf_matrix.toarray(),\n",
        "        columns=tfidf_feature_names,\n",
        "        index=df.index,\n",
        "    )\n",
        "\n",
        "    df_with_tfidf = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "    print(f\"Added {len(tfidf_feature_names)} TF-IDF features.\")\n",
        "    return df_with_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bdb9f84a",
      "metadata": {
        "id": "bdb9f84a"
      },
      "outputs": [],
      "source": [
        "def add_bert_features(df: pd.DataFrame, _train_df: pd.DataFrame, descriptions_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Adds BERT embeddings from book descriptions.\"\"\"\n",
        "    print(\"Adding text features (BERT embeddings)...\")\n",
        "\n",
        "    config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    embeddings_path = config.MODEL_DIR / constants.BERT_EMBEDDINGS_FILENAME\n",
        "\n",
        "    if embeddings_path.exists():\n",
        "        print(f\"Loading cached BERT embeddings from {embeddings_path}\")\n",
        "        embeddings_dict = joblib.load(embeddings_path)\n",
        "    else:\n",
        "        print(\"Computing BERT embeddings (this may take a while)...\")\n",
        "        print(f\"Using device: {config.BERT_DEVICE}\")\n",
        "\n",
        "        if config.BERT_DEVICE == \"cuda\":\n",
        "            torch.cuda.set_per_process_memory_fraction(config.BERT_GPU_MEMORY_FRACTION)\n",
        "            print(f\"GPU memory limited to {config.BERT_GPU_MEMORY_FRACTION * 100:.0f}% of available memory\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.BERT_MODEL_NAME)\n",
        "        model = AutoModel.from_pretrained(config.BERT_MODEL_NAME)\n",
        "        model.to(config.BERT_DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        all_descriptions = descriptions_df[[constants.COL_BOOK_ID, constants.COL_DESCRIPTION]].copy()\n",
        "        all_descriptions[constants.COL_DESCRIPTION] = all_descriptions[constants.COL_DESCRIPTION].fillna(\"\")\n",
        "        unique_books = all_descriptions.drop_duplicates(subset=[constants.COL_BOOK_ID])\n",
        "        book_ids = unique_books[constants.COL_BOOK_ID].to_numpy()\n",
        "        descriptions = unique_books[constants.COL_DESCRIPTION].to_numpy().tolist()\n",
        "\n",
        "        embeddings_dict = {}\n",
        "        num_batches = (len(descriptions) + config.BERT_BATCH_SIZE - 1) // config.BERT_BATCH_SIZE\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx in tqdm(range(num_batches), desc=\"Processing BERT batches\", unit=\"batch\"):\n",
        "                start_idx = batch_idx * config.BERT_BATCH_SIZE\n",
        "                end_idx = min(start_idx + config.BERT_BATCH_SIZE, len(descriptions))\n",
        "                batch_descriptions = descriptions[start_idx:end_idx]\n",
        "                batch_book_ids = book_ids[start_idx:end_idx]\n",
        "\n",
        "                encoded = tokenizer(\n",
        "                    batch_descriptions, padding=True, truncation=True,\n",
        "                    max_length=config.BERT_MAX_LENGTH, return_tensors=\"pt\"\n",
        "                )\n",
        "                encoded = {k: v.to(config.BERT_DEVICE) for k, v in encoded.items()}\n",
        "                outputs = model(**encoded)\n",
        "\n",
        "                attention_mask = encoded[\"attention_mask\"]\n",
        "                attention_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "                sum_embeddings = torch.sum(outputs.last_hidden_state * attention_mask_expanded, dim=1)\n",
        "                sum_mask = torch.clamp(attention_mask_expanded.sum(dim=1), min=1e-9)\n",
        "                mean_pooled = sum_embeddings / sum_mask\n",
        "                batch_embeddings = mean_pooled.cpu().numpy()\n",
        "\n",
        "                for book_id, embedding in zip(batch_book_ids, batch_embeddings, strict=False):\n",
        "                    embeddings_dict[book_id] = embedding\n",
        "\n",
        "                if config.BERT_DEVICE == \"cuda\":\n",
        "                    time.sleep(0.2)\n",
        "\n",
        "        joblib.dump(embeddings_dict, embeddings_path)\n",
        "        print(f\"Saved BERT embeddings to {embeddings_path}\")\n",
        "\n",
        "    df_book_ids = df[constants.COL_BOOK_ID].to_numpy()\n",
        "    embeddings_list = []\n",
        "    for book_id in df_book_ids:\n",
        "        if book_id in embeddings_dict:\n",
        "            embeddings_list.append(embeddings_dict[book_id])\n",
        "        else:\n",
        "            embeddings_list.append(np.zeros(config.BERT_EMBEDDING_DIM))\n",
        "\n",
        "    embeddings_array = np.array(embeddings_list)\n",
        "    bert_feature_names = [f\"bert_{i}\" for i in range(config.BERT_EMBEDDING_DIM)]\n",
        "    bert_df = pd.DataFrame(embeddings_array, columns=bert_feature_names, index=df.index)\n",
        "    df_with_bert = pd.concat([df.reset_index(drop=True), bert_df.reset_index(drop=True)], axis=1)\n",
        "    print(f\"Added {len(bert_feature_names)} BERT features.\")\n",
        "    return df_with_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "46e9395d",
      "metadata": {
        "id": "46e9395d"
      },
      "outputs": [],
      "source": [
        "def handle_missing_values(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Fills missing values using a defined strategy.\"\"\"\n",
        "    print(\"Handling missing values...\")\n",
        "\n",
        "    global_mean = train_df[config.TARGET].mean()\n",
        "    age_median = df[constants.COL_AGE].median()\n",
        "    df[constants.COL_AGE] = df[constants.COL_AGE].fillna(age_median)\n",
        "\n",
        "    if constants.F_USER_MEAN_RATING in df.columns:\n",
        "        df[constants.F_USER_MEAN_RATING] = df[constants.F_USER_MEAN_RATING].fillna(global_mean)\n",
        "    if constants.F_BOOK_MEAN_RATING in df.columns:\n",
        "        df[constants.F_BOOK_MEAN_RATING] = df[constants.F_BOOK_MEAN_RATING].fillna(global_mean)\n",
        "    if constants.F_AUTHOR_MEAN_RATING in df.columns:\n",
        "        df[constants.F_AUTHOR_MEAN_RATING] = df[constants.F_AUTHOR_MEAN_RATING].fillna(global_mean)\n",
        "\n",
        "    if constants.F_USER_RATINGS_COUNT in df.columns:\n",
        "        df[constants.F_USER_RATINGS_COUNT] = df[constants.F_USER_RATINGS_COUNT].fillna(0)\n",
        "    if constants.F_BOOK_RATINGS_COUNT in df.columns:\n",
        "        df[constants.F_BOOK_RATINGS_COUNT] = df[constants.F_BOOK_RATINGS_COUNT].fillna(0)\n",
        "\n",
        "    df[constants.COL_AVG_RATING] = df[constants.COL_AVG_RATING].fillna(global_mean)\n",
        "    df[constants.F_BOOK_GENRES_COUNT] = df[constants.F_BOOK_GENRES_COUNT].fillna(0)\n",
        "\n",
        "    tfidf_cols = [col for col in df.columns if col.startswith(\"tfidf_\")]\n",
        "    for col in tfidf_cols:\n",
        "        df[col] = df[col].fillna(0.0)\n",
        "\n",
        "    bert_cols = [col for col in df.columns if col.startswith(\"bert_\")]\n",
        "    for col in bert_cols:\n",
        "        df[col] = df[col].fillna(0.0)\n",
        "\n",
        "    for col in config.CAT_FEATURES:\n",
        "        if col in df.columns:\n",
        "            if df[col].dtype.name in (\"category\", \"object\") and df[col].isna().any():\n",
        "                df[col] = df[col].astype(str).fillna(constants.MISSING_CAT_VALUE).astype(\"category\")\n",
        "            elif pd.api.types.is_numeric_dtype(df[col].dtype) and df[col].isna().any():\n",
        "                df[col] = df[col].fillna(constants.MISSING_NUM_VALUE)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "36f89a2c",
      "metadata": {
        "id": "36f89a2c"
      },
      "outputs": [],
      "source": [
        "def create_features(\n",
        "    df: pd.DataFrame, book_genres_df: pd.DataFrame, descriptions_df: pd.DataFrame, include_aggregates: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Runs the full feature engineering pipeline.\"\"\"\n",
        "    print(\"Starting feature engineering pipeline...\")\n",
        "    train_df = df[df[constants.COL_SOURCE] == constants.VAL_SOURCE_TRAIN].copy()\n",
        "\n",
        "    if include_aggregates:\n",
        "        df = add_aggregate_features(df, train_df)\n",
        "\n",
        "    df = add_genre_features(df, book_genres_df)\n",
        "    df = add_text_features(df, train_df, descriptions_df)\n",
        "    df = add_bert_features(df, train_df, descriptions_df)\n",
        "    df = handle_missing_values(df, train_df)\n",
        "\n",
        "    for col in config.CAT_FEATURES:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    print(\"Feature engineering complete.\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd21ee9",
      "metadata": {
        "id": "bfd21ee9"
      },
      "source": [
        "## 4.1. Подготовка данных (Prepare Step)\n",
        "Загружаем данные и генерируем базовые признаки (без агрегаций, чтобы избежать утечек)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "be1a3c8d",
      "metadata": {
        "id": "be1a3c8d"
      },
      "outputs": [],
      "source": [
        "def prepare_data() -> None:\n",
        "    \"\"\"Processes raw data and saves prepared features to processed directory.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Data Preparation Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    merged_df, book_genres_df, _, descriptions_df = load_and_merge_data()\n",
        "\n",
        "    # Apply feature engineering WITHOUT aggregates\n",
        "    featured_df = create_features(merged_df, book_genres_df, descriptions_df, include_aggregates=False)\n",
        "\n",
        "    config.PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    processed_path = config.PROCESSED_DATA_DIR / constants.PROCESSED_DATA_FILENAME\n",
        "\n",
        "    print(f\"\\nSaving processed data to {processed_path}...\")\n",
        "    featured_df.to_parquet(processed_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
        "    print(\"Processed data saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "y-Nm4kxxHsJq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Nm4kxxHsJq",
        "outputId": "f870eff1-fceb-4504-8dcd-05c853e3e452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a15d25b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15d25b1",
        "outputId": "0c530af8-22b0-4c64-a082-cb87c4e56eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Data Preparation Pipeline\n",
            "============================================================\n",
            "Loading data...\n",
            "Filtered training data: 268581 -> 156179 rows (only has_read=1)\n",
            "Data loaded. Merging datasets...\n",
            "Merged data shape: (159073, 15)\n",
            "Starting feature engineering pipeline...\n",
            "Adding genre features...\n",
            "Adding text features (TF-IDF)...\n",
            "Loading existing vectorizer from output/models/tfidf_vectorizer.pkl\n",
            "Added 500 TF-IDF features.\n",
            "Adding text features (BERT embeddings)...\n",
            "Loading cached BERT embeddings from output/models/bert_embeddings.pkl\n",
            "Added 768 BERT features.\n",
            "Handling missing values...\n",
            "Feature engineering complete.\n",
            "\n",
            "Saving processed data to /content/drive/MyDrive/data/processed/processed_features.parquet...\n",
            "Processed data saved successfully!\n"
          ]
        }
      ],
      "source": [
        "prepare_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac4dd73",
      "metadata": {
        "id": "fac4dd73"
      },
      "source": [
        "## 4.2. Обучение модели (Train Step)\n",
        "Обучаем LightGBM, используя Temporal Split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d15ececc",
      "metadata": {
        "id": "d15ececc"
      },
      "outputs": [],
      "source": [
        "def train() -> None:\n",
        "    \"\"\"Runs the model training pipeline with temporal split (Memory Optimized).\"\"\"\n",
        "    processed_path = config.PROCESSED_DATA_DIR / constants.PROCESSED_DATA_FILENAME\n",
        "    if not processed_path.exists():\n",
        "        raise FileNotFoundError(f\"Processed data not found at {processed_path}.\")\n",
        "\n",
        "    print(f\"Loading prepared data from {processed_path}...\")\n",
        "\n",
        "    # OPTIMIZATION: Filter at load time or immediately after\n",
        "    try:\n",
        "        featured_df = pd.read_parquet(\n",
        "            processed_path,\n",
        "            engine=\"pyarrow\",\n",
        "            filters=[(constants.COL_SOURCE, '==', constants.VAL_SOURCE_TRAIN)]\n",
        "        )\n",
        "    except Exception:\n",
        "        featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
        "        featured_df = featured_df[featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TRAIN]\n",
        "\n",
        "    print(f\"Loaded {len(featured_df):,} rows. Optimizing memory...\")\n",
        "    featured_df = reduce_mem_usage(featured_df)\n",
        "    gc.collect()\n",
        "\n",
        "    # Use featured_df as train_set\n",
        "    train_set = featured_df\n",
        "\n",
        "    if constants.COL_TIMESTAMP not in train_set.columns:\n",
        "        raise ValueError(f\"Timestamp column '{constants.COL_TIMESTAMP}' not found.\")\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(train_set[constants.COL_TIMESTAMP]):\n",
        "        train_set[constants.COL_TIMESTAMP] = pd.to_datetime(train_set[constants.COL_TIMESTAMP])\n",
        "\n",
        "    print(f\"\\nPerforming temporal split with ratio {config.TEMPORAL_SPLIT_RATIO}...\")\n",
        "    split_date = get_split_date_from_ratio(train_set, config.TEMPORAL_SPLIT_RATIO, constants.COL_TIMESTAMP)\n",
        "    print(f\"Split date: {split_date}\")\n",
        "\n",
        "    train_mask, val_mask = temporal_split_by_date(train_set, split_date, constants.COL_TIMESTAMP)\n",
        "\n",
        "    # Split data\n",
        "    train_split = train_set[train_mask].copy()\n",
        "    val_split = train_set[val_mask].copy()\n",
        "\n",
        "    print(f\"Train split: {len(train_split):,} rows\")\n",
        "    print(f\"Validation split: {len(val_split):,} rows\")\n",
        "\n",
        "    # Cleanup parent dataframe\n",
        "    del train_set, featured_df\n",
        "    gc.collect()\n",
        "\n",
        "    # Verify temporal correctness\n",
        "    max_train_timestamp = train_split[constants.COL_TIMESTAMP].max()\n",
        "    min_val_timestamp = val_split[constants.COL_TIMESTAMP].min()\n",
        "    print(f\"Max train timestamp: {max_train_timestamp}\")\n",
        "    print(f\"Min validation timestamp: {min_val_timestamp}\")\n",
        "\n",
        "    if min_val_timestamp <= max_train_timestamp:\n",
        "        raise ValueError(\"Temporal split validation failed.\")\n",
        "    print(\"✅ Temporal split validation passed\")\n",
        "\n",
        "    print(\"\\nComputing aggregate features on train split only...\")\n",
        "    # Compute aggregates\n",
        "    train_split_with_agg = add_aggregate_features(train_split, train_split)\n",
        "    val_split_with_agg = add_aggregate_features(val_split, train_split)\n",
        "\n",
        "    del train_split, val_split\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"Handling missing values...\")\n",
        "    # Use train_split_with_agg for filling stats (same target stats as train_split)\n",
        "    train_split_final = handle_missing_values(train_split_with_agg, train_split_with_agg)\n",
        "    val_split_final = handle_missing_values(val_split_with_agg, train_split_with_agg)\n",
        "\n",
        "    del train_split_with_agg, val_split_with_agg\n",
        "    gc.collect()\n",
        "\n",
        "    # Define features\n",
        "    exclude_cols = [constants.COL_SOURCE, config.TARGET, constants.COL_PREDICTION, constants.COL_TIMESTAMP]\n",
        "    features = [col for col in train_split_final.columns if col not in exclude_cols]\n",
        "    non_feature_object_cols = train_split_final[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    features = [f for f in features if f not in non_feature_object_cols]\n",
        "\n",
        "    print(f\"Training features: {len(features)}\")\n",
        "\n",
        "    X_train = train_split_final[features]\n",
        "    y_train = train_split_final[config.TARGET]\n",
        "    X_val = val_split_final[features]\n",
        "    y_val = val_split_final[config.TARGET]\n",
        "\n",
        "    # Cleanup before training\n",
        "    del train_split_final, val_split_final\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\nTraining LightGBM model...\")\n",
        "\n",
        "    # Use lgb.Dataset with free_raw_data=True for memory optimization\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=config.CAT_FEATURES, free_raw_data=True)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=config.CAT_FEATURES, reference=train_data, free_raw_data=True)\n",
        "\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=config.EARLY_STOPPING_ROUNDS, verbose=True),\n",
        "        lgb.log_evaluation(period=50),\n",
        "    ]\n",
        "\n",
        "    model = lgb.train(\n",
        "        config.LGB_PARAMS,\n",
        "        train_data,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        valid_names=[\"train\", \"valid\"],\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    # Clip predictions\n",
        "    val_preds = np.clip(val_preds, constants.PREDICTION_MIN_VALUE, constants.PREDICTION_MAX_VALUE)\n",
        "\n",
        "    mae = mean_absolute_error(y_val, val_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    print(f\"Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "    config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = config.MODEL_DIR / config.MODEL_FILENAME\n",
        "    model.save_model(str(model_path))\n",
        "    print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "FNUHkmD_7cy2",
      "metadata": {
        "id": "FNUHkmD_7cy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b6d57a-9054-48ae-88eb-d0baff48f6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading prepared data from /content/drive/MyDrive/data/processed/processed_features.parquet...\n",
            "Loaded 156,179 rows. Optimizing memory...\n",
            "Memory usage of dataframe is 1065.40 MB\n",
            "Memory usage after optimization is: 765.87 MB\n",
            "Decreased by 28.1%\n",
            "\n",
            "Performing temporal split with ratio 0.8...\n",
            "Split date: 2020-09-27 16:17:15\n",
            "Train split: 124,944 rows\n",
            "Validation split: 31,235 rows\n",
            "Max train timestamp: 2020-09-27 16:17:15\n",
            "Min validation timestamp: 2020-09-27 16:51:29\n",
            "✅ Temporal split validation passed\n",
            "\n",
            "Computing aggregate features on train split only...\n",
            "Adding aggregate features...\n",
            "Adding aggregate features...\n",
            "Handling missing values...\n",
            "Handling missing values...\n",
            "Handling missing values...\n",
            "Training features: 1284\n",
            "\n",
            "Training LightGBM model...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's rmse: 2.78165\tvalid's rmse: 2.92146\n",
            "Early stopping, best iteration is:\n",
            "[3]\ttrain's rmse: 2.79733\tvalid's rmse: 2.90368\n",
            "\n",
            "Evaluating on validation set...\n",
            "Validation RMSE: 2.9037, MAE: 2.1344\n",
            "Model saved to output/models/lgb_model.txt\n"
          ]
        }
      ],
      "source": [
        "# ЗАПУСК ОБУЧЕНИЯ\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcksEWhm7cy2",
      "metadata": {
        "id": "DcksEWhm7cy2"
      },
      "source": [
        "## 4.3. Генерация предсказаний (Predict Step)\n",
        "Генерируем сабмит для тестовой выборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "MHNsyDvs7cy2",
      "metadata": {
        "id": "MHNsyDvs7cy2"
      },
      "outputs": [],
      "source": [
        "def predict() -> None:\n",
        "    \"\"\"Generates and saves predictions for the test set.\"\"\"\n",
        "    processed_path = config.PROCESSED_DATA_DIR / constants.PROCESSED_DATA_FILENAME\n",
        "    if not processed_path.exists():\n",
        "        raise FileNotFoundError(f\"Processed data not found at {processed_path}.\")\n",
        "\n",
        "    print(f\"Loading prepared data from {processed_path}...\")\n",
        "    featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
        "\n",
        "    train_set = featured_df[featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TRAIN].copy()\n",
        "    test_set = featured_df[featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TEST].copy()\n",
        "\n",
        "    print(\"\\nComputing aggregate features on all train data...\")\n",
        "    test_set_with_agg = add_aggregate_features(test_set.copy(), train_set)\n",
        "\n",
        "    print(\"Handling missing values...\")\n",
        "    test_set_final = handle_missing_values(test_set_with_agg, train_set)\n",
        "\n",
        "    exclude_cols = [constants.COL_SOURCE, config.TARGET, constants.COL_PREDICTION, constants.COL_TIMESTAMP]\n",
        "    features = [col for col in test_set_final.columns if col not in exclude_cols]\n",
        "    non_feature_object_cols = test_set_final[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    features = [f for f in features if f not in non_feature_object_cols]\n",
        "\n",
        "    X_test = test_set_final[features]\n",
        "\n",
        "    model_path = config.MODEL_DIR / config.MODEL_FILENAME\n",
        "    print(f\"\\nLoading model from {model_path}...\")\n",
        "    model = lgb.Booster(model_file=str(model_path))\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    test_preds = model.predict(X_test)\n",
        "    clipped_preds = np.clip(test_preds, constants.PREDICTION_MIN_VALUE, constants.PREDICTION_MAX_VALUE)\n",
        "\n",
        "    submission_df = test_set[[constants.COL_USER_ID, constants.COL_BOOK_ID]].copy()\n",
        "    submission_df[constants.COL_PREDICTION] = clipped_preds\n",
        "\n",
        "    config.SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    submission_path = config.SUBMISSION_DIR / constants.SUBMISSION_FILENAME\n",
        "\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"\\nSubmission file created at: {submission_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "sXbiVX7k7cy2",
      "metadata": {
        "id": "sXbiVX7k7cy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3611d548-b7c4-43dc-dd2b-4b1bf16aada3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading prepared data from /content/drive/MyDrive/data/processed/processed_features.parquet...\n",
            "\n",
            "Computing aggregate features on all train data...\n",
            "Adding aggregate features...\n",
            "Handling missing values...\n",
            "Handling missing values...\n",
            "\n",
            "Loading model from output/models/lgb_model.txt...\n",
            "Generating predictions...\n",
            "\n",
            "Submission file created at: output/submissions/submission.csv\n"
          ]
        }
      ],
      "source": [
        "predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-AeQDvq7cy2",
      "metadata": {
        "id": "n-AeQDvq7cy2"
      },
      "source": [
        "## 4.4. Валидация формата (Validate Step)\n",
        "Проверяем, что файл сабмита соответствует требованиям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bPMJBd2g7cy2",
      "metadata": {
        "id": "bPMJBd2g7cy2"
      },
      "outputs": [],
      "source": [
        "def validate() -> None:\n",
        "    \"\"\"Validates the structure and format of the submission file.\"\"\"\n",
        "    print(\"Validating submission file...\")\n",
        "    try:\n",
        "        test_df = pd.read_csv(config.RAW_DATA_DIR / constants.TEST_FILENAME)\n",
        "        sub_df = pd.read_csv(config.SUBMISSION_DIR / constants.SUBMISSION_FILENAME)\n",
        "\n",
        "        assert len(sub_df) == len(test_df), f\"Length mismatch. Expected {len(test_df)}, got {len(sub_df)}.\"\n",
        "        assert not sub_df[constants.COL_PREDICTION].isna().any(), \"Missing values in prediction.\"\n",
        "\n",
        "        test_keys = test_df[[constants.COL_USER_ID, constants.COL_BOOK_ID]].copy().set_index([constants.COL_USER_ID, constants.COL_BOOK_ID])\n",
        "        sub_keys = sub_df[[constants.COL_USER_ID, constants.COL_BOOK_ID]].copy().set_index([constants.COL_USER_ID, constants.COL_BOOK_ID])\n",
        "        assert test_keys.index.equals(sub_keys.index), \"User/Book pairs do not match test set.\"\n",
        "\n",
        "        assert sub_df[constants.COL_PREDICTION].between(constants.PREDICTION_MIN_VALUE, constants.PREDICTION_MAX_VALUE).all(), \"Predictions out of range.\"\n",
        "\n",
        "        print(\"\\nValidation successful! Submission file is valid.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Validation failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Ph1Hi3wT7cy2",
      "metadata": {
        "id": "Ph1Hi3wT7cy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891f1cbb-76a1-4345-d19d-1d15247f41f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating submission file...\n",
            "\n",
            "Validation successful! Submission file is valid.\n"
          ]
        }
      ],
      "source": [
        "validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zrvRYTXV7cy2",
      "metadata": {
        "id": "zrvRYTXV7cy2"
      },
      "source": [
        "## 5. Оценка качества (Evaluation)\n",
        "Локальная проверка качества, если вы создали test выборку и для неё файл с правильными ответами solution.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "64zUKbvK7cy3",
      "metadata": {
        "id": "64zUKbvK7cy3"
      },
      "outputs": [],
      "source": [
        "def validate_submission_format_for_eval(df: pd.DataFrame, solution_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Validate submission file format against solution.\"\"\"\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Submission file is empty\")\n",
        "\n",
        "    required_cols = {\"user_id\", \"book_id\", \"rating_predict\"}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        missing = required_cols - set(df.columns)\n",
        "        raise ValueError(f\"Missing required columns: {missing}. Expected: {required_cols}\")\n",
        "\n",
        "    if df.shape[0] != solution_df.shape[0]:\n",
        "        raise ValueError(f\"Row count mismatch: {df.shape[0]} in submission, {solution_df.shape[0]} expected\")\n",
        "\n",
        "def calculate_stage1_metrics(merged_df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"Calculate RMSE, MAE, and Score metrics.\"\"\"\n",
        "    if merged_df.empty:\n",
        "        return {\"Score\": 0.0, \"RMSE\": 0.0, \"MAE\": 0.0}\n",
        "\n",
        "    y_true = merged_df[\"rating\"]\n",
        "    y_pred = merged_df[\"rating_predict\"].clip(0, 10)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "    range_width = 10.0\n",
        "    mae_norm = mae / range_width\n",
        "    rmse_norm = rmse / range_width\n",
        "    score = 1 - (0.5 * rmse_norm + 0.5 * mae_norm)\n",
        "\n",
        "    return {\"Score\": score, \"RMSE\": rmse, \"MAE\": mae}\n",
        "\n",
        "def evaluate_submission(submission_path: str, solution_path: str) -> Optional[Dict[str, float]]:\n",
        "    \"\"\"Main evaluation function.\"\"\"\n",
        "    print(f\"Evaluating {submission_path} against {solution_path}...\")\n",
        "    try:\n",
        "        submission = pd.read_csv(submission_path)\n",
        "        solution = pd.read_csv(solution_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File not found: {e.filename}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        validate_submission_format_for_eval(submission, solution)\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}\")\n",
        "        return None\n",
        "\n",
        "    solution_public = solution[solution[\"stage\"] == \"public\"].copy()\n",
        "    solution_private = solution[solution[\"stage\"] == \"private\"].copy()\n",
        "\n",
        "    public_merged = submission.merge(solution_public, on=[\"user_id\", \"book_id\"], how=\"inner\")\n",
        "    private_merged = submission.merge(solution_private, on=[\"user_id\", \"book_id\"], how=\"inner\")\n",
        "\n",
        "    public_metrics = calculate_stage1_metrics(public_merged)\n",
        "    private_metrics = calculate_stage1_metrics(private_merged)\n",
        "\n",
        "    print(\"--- Public ---\")\n",
        "    for metric, value in public_metrics.items():\n",
        "        print(f\"{metric}: {value:.6f}\")\n",
        "\n",
        "    print(\"\\n--- Private ---\")\n",
        "    for metric, value in private_metrics.items():\n",
        "        print(f\"{metric}: {value:.6f}\")\n",
        "\n",
        "    return {\n",
        "        \"public_score\": public_metrics[\"Score\"],\n",
        "        \"private_score\": private_metrics[\"Score\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "RGQ5iuCG7cy3",
      "metadata": {
        "id": "RGQ5iuCG7cy3"
      },
      "outputs": [],
      "source": [
        "solution_file = \"/content/solution.csv\"\n",
        "submission_file = config.SUBMISSION_DIR / constants.SUBMISSION_FILENAME\n",
        "if os.path.exists(solution_file):\n",
        "  evaluate_submission(submission_file, solution_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}